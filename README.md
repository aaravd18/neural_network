- Created a neural network from scratch, no libraries used apart from numpy
- Implemented back propagation, stochastic gradient descent, and experimented with different activation functions like ReLU, leaky ReLU, and sigmoid
- Credit to 3b1b and Michael Nielsen's "Neural Networks and Deep Learning" book for teaching the relevant mathematics, logic, and some code inspiration
